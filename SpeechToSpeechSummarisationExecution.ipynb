{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to Speech Summarisation : Execution Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Requirements and Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import spacy\n",
    "import scipy.io\n",
    "import math\n",
    "import pickle \n",
    "import Levenshtein\n",
    "import string  \n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from gensim.summarization.summarizer import summarize as extractive_sum\n",
    "from rouge import Rouge \n",
    "from allennlp.data.tokenizers import Token, Tokenizer, SpacyTokenizer\n",
    "from gtts import gTTS\n",
    "from IPython.display import Audio\n",
    "rouge = Rouge()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Acoustic Prominence Scorer\n",
    "with open('best_svm.sav', 'rb') as pickle_file:\n",
    "    clf = pickle.load(pickle_file)\n",
    "\n",
    "def log_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.log(e_x / e_x.sum())\n",
    "        \n",
    "def acoustic_prominence_scorer(pre_aligned_acoustic_features,alignment): \n",
    "    raw_scores = clf.predict(pre_aligned_acoustic_features)\n",
    "    scored_sequence = {}\n",
    "    appearances = {}\n",
    "    for x in alignment :\n",
    "        if x[0] != '<eps>':\n",
    "            start = math.ceil((x[1])/10)-1\n",
    "            gap =  math.floor((x[2]/10- (x[2]%1)))+1\n",
    "            end= start+gap\n",
    "            relevant_scores = raw_scores[start:end]\n",
    "            mean_score = sum(relevant_scores)/len(relevant_scores)\n",
    "            if x[0] in appearances:\n",
    "                new_freq = appearances[x[0]]+1\n",
    "                appearances[x[0]] = new_freq\n",
    "                scored_sequence[x[0]] = ((new_freq-1) * appearances[x[0]] + mean_score) / new_freq\n",
    "            else:\n",
    "                appearances[x[0]] = 1\n",
    "                scored_sequence[x[0]] = mean_score\n",
    "#     scored_sequence = dict(zip(scored_sequence.keys(), log_softmax(list(scored_sequence.values()))))\n",
    "    return scored_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alignment and acoustic features from the disk\n",
    "ALIGNMENTPATH = \"alignments/\"\n",
    "FEATUREPATH = \"./acoustic_feats_170520/\"\n",
    "exploretable = pd.read_pickle(\"./exploretable16052020.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import summarisation model\n",
    "\n",
    "import nlpete.training.metrics\n",
    "import nlpete.data.dataset_readers\n",
    "from nlpete.models.copynet import CopyNet\n",
    "from allennlp.data.fields.text_field import TextFieldTensors\n",
    "from overrides import overrides\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.data import DatasetReader\n",
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.data import Instance\n",
    "from nlpete.data.dataset_readers import (\n",
    "    CopyNetDatasetReader,\n",
    ") \n",
    "from allennlp.predictors import Predictor\n",
    "import warnings\n",
    "class CopyNetPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    Predictor for the CopyNet model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "        warnings.warn(\n",
    "            \"The 'copynet' predictor has been deprecated in favor of \"\n",
    "            \"the 'seq2seq' predictor.\",\n",
    "            DeprecationWarning,\n",
    "        )\n",
    "        \n",
    "    def predict(self, source: str,acoustic_data: str) -> JsonDict:\n",
    "        return self.predict_json({\"source_string\": source,\"acoustic_data\":acoustic_data})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        source = json_dict[\"source_string\"]\n",
    "        acoustic_data = json_dict[\"acoustic_data\"]\n",
    "        return self._dataset_reader.text_to_instance(source,acoustic_data)\n",
    "\n",
    "archive = load_archive('./absummodel3.tar.gz')\n",
    "predictor = CopyNetPredictor.from_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction \n",
    "chosen_id = None\n",
    "while chosen_id is None:\n",
    "    input_value = int(input(\"Which ID would you like to summarise? \"))\n",
    "    try:\n",
    "        # try and convert the string input to a number\n",
    "        if 0 <= int(input_value) < 500:\n",
    "            chosen_id = int(input_value)\n",
    "    except ValueError:\n",
    "        # tell the user off\n",
    "        print(\"{input} is not a number, please enter a number only\".format(input=input_value))\n",
    "\n",
    "youtube_id = str(exploretable.iloc[chosen_id]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_start_indexes(alignment,summary):\n",
    "    indexes = []\n",
    "    for idx, i in enumerate(alignment):\n",
    "        if (summary[0] == i[0] ):\n",
    "            indexes.append(idx)\n",
    "    return indexes\n",
    "\n",
    "def find_end_indexes(alignment,summary):\n",
    "    lastword = summary[-1]\n",
    "    if lastword in string.punctuation: \n",
    "        return find_end_indexes(alignment,summary[:-1])\n",
    "    indexes = [index for index, m in enumerate(alignment) if re.match(f\"\\('{lastword}', \\d+, \\d+\\)\", str(m))]\n",
    "    if (indexes != []):\n",
    "        return indexes\n",
    "    else:\n",
    "        return find_end_indexes(alignment,list(reversed(summary))[:-1])\n",
    "\n",
    "def mode_1_summary(youtube_id):\n",
    "    original_transcription = exploretable.loc[exploretable['id'] == youtube_id]['tran'].values[0]\n",
    "    alignment_data =  eval(open(f'{ALIGNMENTPATH}{youtube_id}.txt', \"r\").read())\n",
    "    ground_truth_summary = exploretable.loc[exploretable['id'] == youtube_id]['desc'].values[0]\n",
    "    print(ground_truth_summary)\n",
    "    try:\n",
    "        summary = extractive_sum(original_transcription,word_count=40)\n",
    "    except:\n",
    "        return \"Extractive Summarisation Failed\"\n",
    "    start_indexes = find_start_indexes(alignment_data,summary.split(\" \"))      \n",
    "    end_indexes = find_end_indexes(alignment_data,summary.split(\" \"))    \n",
    "    editdistance = 99999\n",
    "    mini = None\n",
    "    minj = None\n",
    "    for i in start_indexes:\n",
    "        for j in end_indexes:\n",
    "            candidate = alignment_data[i:j+1]\n",
    "            candidate = [x[0] for x in candidate if x[0] != '<eps>']\n",
    "            candidate = \" \".join(candidate)\n",
    "            candidatedistance = Levenshtein.distance(candidate,summary)\n",
    "            if ( candidatedistance < editdistance):\n",
    "                editdistance = candidatedistance\n",
    "                mini = i\n",
    "                minj = j\n",
    "    audiostart = alignment_data[mini][1]*10\n",
    "    audioend= (alignment_data[minj][1]+alignment_data[minj][2])*10 +100\n",
    "    print(summary)\n",
    "    originalutterance = AudioSegment.from_wav(f'./speech_audios/{youtube_id}.wav')\n",
    "    originalutterance.export(\"4.mp3\", format=\"wav\")\n",
    "    sumaudio = originalutterance[audiostart:audioend]\n",
    "    sumaudio.export(\"4a.mp3\", format=\"wav\")\n",
    "    play(sumaudio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "def mode_2_summary(youtube_id):\n",
    "    original_transcription = exploretable.loc[exploretable['id'] == youtube_id]['tran'].values[0]\n",
    "    ground_truth_summary = exploretable.loc[exploretable['id'] == youtube_id]['desc'].values[0]\n",
    "    results = predictor.predict(original_transcription,[])\n",
    "    print(f\"The ground truth summary is: {ground_truth_summary}\")\n",
    "    sentences = []\n",
    "    for predictions in results['predicted_tokens']:\n",
    "        sentences.append(\"\".join(predictions))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "def mode_3_summary(youtube_id):\n",
    "    original_transcription = exploretable.loc[exploretable['id'] == youtube_id]['tran'].values[0]\n",
    "    ground_truth_summary = exploretable.loc[exploretable['id'] == youtube_id]['desc'].values[0]\n",
    "    alignment_data =  eval(open(f'{ALIGNMENTPATH}{youtube_id}.txt', \"r\").read())\n",
    "    pre_aligned_acoustic_features = preprocessing.scale(scipy.io.loadmat(FEATUREPATH+youtube_id)['ret'])\n",
    "    aligned_acoustic_data =  acoustic_prominence_scorer(pre_aligned_acoustic_features,alignment_data)\n",
    "    results = predictor.predict(original_transcription,aligned_acoustic_data)\n",
    "    print(f\"The ground truth summary is: {ground_truth_summary}\")\n",
    "    sentences = []\n",
    "    for predictions in results['predicted_tokens']:\n",
    "        sentences.append(\"\".join(predictions))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_1_summary(youtube_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_2_summary(youtube_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_3_summary(youtube_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sum_mode_3 = mode_3_summary(youtube_id)[1]\n",
    "tts = gTTS(text_sum_mode_3)\n",
    "\n",
    "display(Audio('4c.mp3', rate=8000, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the evaluation o \n",
    "url = 'http://56a58cf2ee29.ngrok.io/file-upload'\n",
    "files =  {'file': open(f'./speech_audios/{youtube_id}.wav','rb')}\n",
    "myobj = {\n",
    "    'summary': 'This is a test of the API, hopefully it will work better',\n",
    "}\n",
    "\n",
    "x = requests.post(url, data = myobj,files = files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_return = json.loads(x.text)\n",
    "returned_wav = json.loads(api_return['ret_wav'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Audio(returned_wav, rate=8000, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tts = gTTS(ground_truth_summary)\n",
    "tts.save('temp.mp3')\n",
    "from IPython.display import Audio\n",
    "display(Audio('temp.mp3', rate=8000, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('FYPenv': conda)",
   "language": "python",
   "name": "python37564bitfypenvconda60f1d227ddc44fa2a53c258bf33a3203"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
